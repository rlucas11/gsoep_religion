---
title: "On the Robustness of Reciprocal Associations Between Personality and Religiosity in a German Sample"
short title: "Personality and Religion"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
  - name: Julia Rohrer
    affiliation: 2
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"
  - id: 2
    institution: "Department of Psychology, University of Leipzig"


abstract: |
  Here's an abstract. 
  
  
keywords: "personality, religiosity, cross-lagged panel model"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
   - r-references.bib
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes

---

```{r setup, include=FALSE}
## Load packages
library(lavaan)
library(tidyverse)
library(knitr)
library(papaja)
library(directlabels)
library(gridExtra)
library(ggplot2)
library(stringr)

## Set options
options(knitr.kable.NA='')

```


A common goal in personality research is to identify robust associations between personality characteristics and consequential behaviors and outcomes. Identifying these associations allows researchers to develop and test hypotheses that inform personality theories. For instance, researchers may study the links between a trait like conscientiousness and an outcome like job achievement. A greater understanding of the processes underlying this association can inform theories about conscientiousness. In addition, this research could provide practical guidance for those seeking to improve achievement levels. Moreover, a consideration of the reverse causal direction---understanding whether achievement experiences impact trait levels---can inform theories of personality development and change. Thus, studies that examine the processes underlying such links have great potential further the understanding of how personality characteristics shape peoples lives, and how life experiences shape personality. 

Recently, @entringer_big_2023 conducted such an examination, investigating the links between the Big Five personality traits and religiosity in a very large German sample. This research was motivated both by prior theories meant to explain how personality can shape religiosity and by theories that posit that religiosity can affect personality. For instance, a niche-picking perspective suggests that people who have personality traits that are consistent with the behaviors that are typically exhibited in religious contexts should gravitate towards these religious contexts. In addition, the Sociocultural Norm Perspective (Eck & Gebauer, 2022) posits that personality traits like agreeableness, conscientiousness, and (low) openness to experience produce normative behaviors; when religiosity is normative in a culture, then these traits should cause greater religiosity. In contrast, complementary theories suggest that religiosity itself can impact these same traits, as religious contexts also promote or even enforce behaviors and views that are consistent with traits like agreeableness and conscientiousness. 

To test these ideas, @entringer_big_2023 relied on a widely used approach for examining reciprocal causal effects in panel data: the cross-lagged panel model (CLPM, @heise_causal_1970). In the CLPM, each variable (in this case, personality and religiosity) at each occasion is predicted from the same variables assessed at prior waves. With some assumptions, lagged associations from one variable to the other (e.g., from Time 1 personality to Time 2 religiosity) can be interpreted as causal effects. @entringer_big_2023 found that agreeableness, openness, and conscientiousness prospectively predicted changes in religiosity (at least in some contexts), whereas religiosity predicted changes in agreeableness and openness (again, in some contexts). Moreover, the religiosity of the region in which respondents lived moderated the links between some personality traits and religion. 

Entringer et al.'s study had a number of desirable features that make it especially well-suited to examining questions about reciprocal associations between personality and religiosity. First, the authors used a very large panel study with four waves of assessment over a period of twelve years. These features should contribute to the robustness of the results. Moreover, the authors used a sophisticated latent-variable version of the CLPM that accounts for measurement error, which can help reduce the likelihood of spurious lagged associations [@lucas_why_2023]. In addition, the authors examined these associations separately in different German federal states that varied in overall religiosity, which allowed them to examine theoretically relevant contextual moderators of these associations. Finally, the authors conducted many robustness checks to support their primary findings. 

## Concerns About Robustness

Despite these strengths, however, there are reasons to be concerned about the robustness of the support for reciprocal causal effects between personality and religiosity. The first issue concerns the size of the effects that @entringer_big_2023 found. The authors foreshadow this concern early in the paper, noting that lagged effects are typically much smaller than cross-sectional effects and that prior cross-sectional correlations should provide an upper bound on the size of any lagged associations. Because the correlations between personality and religiosity tend to be small (e.g., around .19 in their review), the lagged effects should be even smaller. Indeed, the observed lagged effects in their study were quite small, with maximum standardized regression coefficients of .039. The estimated moderator effects they found were similarly small in size.

Although we agree that small effects can sometimes be important, such effects provide challenges for interpretation. First, just because such effects can be important in some contexts does not mean that they are always important; justification for why a particular small effect is important is needed. Moreover, one common defense of the importance of small effects is that these effects accumulate over time. However, this should mean that aggregated between-person correlations should themselves be reasonably large in size. @entringer_big_2023 did not report zero-order correlations either within waves or after aggregating personality and religiosity across the 12-year period. Perhaps more importantly, very small effect sizes can be problematic because subtle model misspecification or residual confounding can lead to small effects. This makes it difficult to distinguish true effects from model misspecification or confounding when effects are small. Additional concerns about model misspecification and confounding could easily lead to the size of effects found in this study. 

For instance, although Entringer et al.'s decision to model latent personality factors when examining reciprocal associations has the desirable feature of removing measurement error, it also comes with a cost in terms of model complexity. Although researchers might hope that the items of their measures load cleanly on the factor to which the item belongs (and not to any other), this is not always the case in practice. In such cases, allowing for secondary loadings may be necessary to improve model fit, though questions can remain about whether such post hoc modifications capitalize on chance. If decisions about the measurement model affect the structural features of the model, then concerns about the robustness of estimates (especially estimates that are very small in size) can be raised. In the current study, we compare the latent-variable model with a complex measurement-model specification from @entringer_big_2023 to a simpler observed-variable model that includes only the observed mean scores for each personality trait measure. 

A second concern is that @entringer_big_2023 chose to model all five traits simultaneously when predicting changes in religiosity. Although the Big Five traits are hypothesized to be relatively independent, in practice they are not. Thus, when modeling all traits simultaneously, estimated paths from personality to religiosity reflect associations that persist after controlling for all other personality traits. The decision to control for correlated variables comes with interpretational challenges, however, as the association can only be interpreted as an association between religiosity and the variance that is not shared with other traits [@lynam_perils_2006]. The conceptual connections between this residualized variable and the hypothetical construct it is meant to assess are not always clear. Although @entringer_big_2023 justified their decision to simultaneously model all five traits by noting that this decision is consistent with prior research, a more substantive justification would be preferable. Because of the interpretational challenges, our preference is to interpret unadjusted associations; but at the very least, robustness across modeling choices is important to consider. 

The final concern is potentially the most consequential. When examining reciprocal effects, @entringer_big_2023 relied solely on the traditional CLPM with a single lagged association with the prior wave. However, @hamaker_critique_2015 showed that the CLPM results in biased lagged associations when stable-trait variance exists in the measures being modeled. Recently, @lucas_why_2023 used simulations to show that this problem is quite severe; spurious lagged associations can be found as often as 100% of the time in realistic scenarios (e.g., when there is just a moderate amount of stable-trait variance, when sample sizes are moderate to large, and when multiple waves of assessment are included)[^orth]. The size of the bias found in these simulations is high relative to the size of effects reported by @entringer_big_2023. Notably, two strengths of Entringer et al.'s study (the very large sample size and the use of a four-wave design) increase the likelihood of finding spurious lagged effects. 

[^orth]: Recently, @orth_testing_2021 defended the CLPM against concerns raised by @hamaker_critique_2015 and others. Their defense focused on the interpretation of the between-person and within-person associations that are modeled in the alternatives to the CLPM. Specifically, they argued that the CLPM can test "between-person prospective effects," while the alternatives to the CLPM do not. As explained in @lucas_why_2023, we disagree that the between-person prospective effects that Orth et al. hope to assess are clearly defined, and we disagree that the CLPM tests them [see @lucas_why_2023 for a discussion]. More importantly, this defense ignores the fact that a critical assumption of the CLPM is that no additional source of stability in the outcome measures exists beyond those that are included in the model (i.e., the autoregressive effect reflected in the stability of a variable over time and the lagged effect of one variable on the other at a later time). If stable-trait variance exists, then this assumption is violated, invalidating the interpretation of the lagged paths as causal effects [@heise_causal_1970]. 

## The Present Study

Although @entringer_big_2023 took many steps to ensure the robustness of their results, additional concerns can be raised about effect sizes, model complexity, the decision to simultaneously model all five personality traits, and---most importantly---the decision to use a traditional CLPM instead of a model that accounts for additional sources of stability. The goal of the current analysis is to test the robustness of these results to alternative specifications. 

We first simply examine the correlations between religiosity and each of the Big Five traits within each wave and aggregated across all waves. This provides a simple index of effect size that helps establish whether any lagged causal effects accumulate over time. Next, we test a series of models that examine the robustness of the results reported in @entringer_big_2023. Specifically, after first replicating their results, we then test various combinations of three modifications. We compare models where the Big Five traits are modeled as latent-traits (using the same measurement model in the original paper, including secondary loadings) versus when they are modeled as observed variables. Next, we compare models where the Big Five traits are entered simultaneously as predictors to those where separate models are run for each trait individually. Finally, we compare the CLPM to the random-intercept cross-lagged panel model [RI-CLPM, @hamaker_critique_2015], which includes a random intercept to account for stable-trait variance. Combining each pair of comparisons results in eight separate models, with results for each of the Big Five traits. In addition, @entringer_big_2023 examined the moderating contextual effect of state-level religiosity. We also test these moderating effects in each of the eight models for each of the Big Five traits. The results will then be compared for robustness.

# Methods

This paper uses data from the German Socio-Economic Panel (SOEP) study, which assessed the Big Five personality traits and religiosity four times, at four-year intervals. The inclusion of four waves of assessment allows for the use of both the CLPM and the more complex RI-CLPM. @entringer_big_2023 provided detailed code for their analyses, however, they did not provide code to extract and clean data from the raw data files. As a test of the reproducibility of the analyses from the description provided in the text, we developed our own code for extracting and cleaning data. Despite our best attempts, the exact number of participants included in the final samples, and the precise estimates from the original model could not be perfectly reproduced. However, both the final sample sizes and results are quite close to the original, and all substantive conclusions from the original paper were supported. Thus we proceeded with our robustness checks using the sample we extracted. Our full code for extracting, cleaning, and analyzing these data is available at [https://github.com/rlucas11/gsoep_religion](https://github.com/rlucas11/gsoep_religion)). 

## Participants

The SOEP is a long-running panel study 


# Results

We first examine the size of the correlations between each Big Five personality trait and religiosity. To allow any short-term associations to accumulate over time, we focus on the correlations between scores aggregated across all available assessments. Specifically, we computed each person's average score for each personality trait across all available waves and the average religiosity score across all waves. We then examined the correlations among these variables (a) in each state, (b) by pooling the within-state associations, and (c) by examining the raw correlation among all participants, ignoring residency. These correlations are reported in table \@ref(tab:correlations). 


```{r correlations, echo=FALSE, message=FALSE, warning=FALSE}

################################################################################
## Correlations by state
################################################################################
load("results/correlationsByState.RData")
states <- read_csv("info/states.csv")

## Initialize data frame
corTab <- data.frame(
    State = numeric(),
    Agr = numeric(),
    Con = numeric(),
    Ext = numeric(),
    Neu = numeric(),
    Opn = numeric(),
    Religiosity = numeric(),
    N = numeric()
)

## Extract results
for (i in 1:16) {
    corTab[i, ] <- c(
        states[i, 2],
        out$r[i][[1]][6, 1:5],
        out$mean[i, 6],
        out$n[i, 7]
    )
}

corTab[17,1] <- "Pooled Within"
corTab[17,2:6] <- out$rwg[6, 1:5]
corTab[17,8] <- sum(corTab[1:16, 8])
corTab[18,1] <- "Raw Correlation"
corTab[18,2:6] <- out$raw[6, 1:5]
corTab[18,8] <- sum(corTab[1:16, 8])

corTab$N <- as.character(corTab$N)


papaja::apa_table(corTab,
                  midrules=c(16),
                  align=rep("r", 8),
                  format.args=list(na_string=""),
                  col_spanners=list(`Correlation with Religiosity`=c(2, 6)),
                  caption="Within-state correlations between each personality trait and religiosity.",
                  note="Agr = Agreableness; Con = Conscientiousness; Ext = Extraversion; Neu = Neuroticism; Opn = Openness. Mean religiosity and sample sizes are presented in the rightmost columns.")


```

As can be seen in this table, even after aggregating over a 12-year period, all correlations are quite small. The largest correlation for the full sample is between religiosity and agreeableness, which is just `r corTab[18,2]` The largest correlation among the `r 5*16` tested (including those from the full sample and pooled within-state associations) was just `r max(corTab[1:18, 2:6])`. Thus, even when allowing an effect to accumulate over a 12-year period, correlations between personality and religiosity are quite small. We next turn to models that examine the reciprocal associations between personality and religiosity. 

## Full Sample Analyses

In their original paper, @entringer_big_2023 focused on results from each federal state individually and on the meta-analytic averages of these state-level associations. This allowed them to examine whether results varied across states and specifically to test whether state-level religion moderated associations between religiosity and personality. Although we also use this approach for some of our analyses, we first present model comparisons in the full sample, ignoring state of residence. We do this for three reasons. First, as we will show, the estimates from the full sample mostly replicate the meta-analytic averages across the 14 analyzed states, and focusing on one sample instead of 14 (or 16) allows for clearer model comparisons. Second (and most importantly), in the original analyses, two states were dropped because of small sample sizes that resulted in problems with model convergence. When testing additional models, the specific states in which estimation problems emerge differ across models, which means that any differences that result could be due to differences in the models or differences in which states provide estimates. Finally, by ignoring the state-level structure, no participants needed to be excluded due to residence in states for which sample sizes were small. Again, we note that we eventually do discuss results in each state individually in the next section. 

The models we tested compare estimates across three dichotomous factors: (1) The original CLPM versus the potentially more appropriate RI-CLPM, (2) Models that include latent personality traits (with a complex measurement model) versus models that include only a single observed mean score for each trait, and (c) models that include all five traits simultaneously (and hence control for correlations among traits when predicting religiosity) versus models that include just one trait per model. The combination of these factors results in eight sets of estimates for each personality trait. Fit indices for all models are presented in Table \@ref(tab:fullFit). Consistent with @entringer_big_2023, we report Chi-square with degrees of freedom and p-value, along with CFI, RMSEA, and SRMR. 

```{r fullFit, echo=FALSE, message=FALSE, warning=FALSE}

## Change this for final paper
location <- "testResults"

## Initialize df
fit <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

## Load results for each set of models
load(paste0(location, "/clpm.latent.results.RData"))
fit[1, 1:6] <- clpm.latent.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[1, 7:10] <- c(
        NA,
        "clpm",
        "latent",
        "all")

load(paste0(location, "/riclpm.latent.results.RData"))
fit[2, 1:6] <- riclpm.latent.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[2, 7:10] <- c(
    NA,
    "riclpm",
    "latent",
    "all"
)


load(paste0(location, "/clpm.observed.results.RData"))
fit[3, 1:6] <- clpm.observed.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[3, 7:10] <- c(
    NA,
    "clpm",
    "observed",
    "all"
)

load(paste0(location, "/riclpm.observed.results.RData"))
fit[4, 1:6] <- riclpm.observed.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[4, 7:10] <- c(
    NA,
    "riclpm",
    "observed",
    "all"
)

## List traits for loop in single-trait models
traits <- c("agr", "cns", "ext", "neu", "opn")

load(paste0(location, "/single.clpm.latent.results.RData"))
c.l.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    c.l.s[i, 1:6] <- single.clpm.latent.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    c.l.s[i, 7] <- traits[i]
    c.l.s[i, 8] <- "clpm"
    c.l.s[i, 9] <- "latent"
    c.l.s[i, 10] <- "single"
}
    

load(paste0(location, "/single.riclpm.latent.results.RData"))
r.l.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    r.l.s[i, 1:6] <- single.riclpm.latent.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    r.l.s[i, 7] <- traits[i]
    r.l.s[i, 8] <- "riclpm"
    r.l.s[i, 9] <- "latent"
    r.l.s[i, 10] <- "single"
}


load(paste0(location, "/single.clpm.obs.results.RData"))
c.o.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    c.o.s[i, 1:6] <- single.clpm.obs.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    c.o.s[i, 7] <- traits[i]
    c.o.s[i, 8] <- "clpm"
    c.o.s[i, 9] <- "observed"
    c.o.s[i, 10] <- "single"
}


load(paste0(location, "/single.riclpm.obs.results.RData"))
r.o.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    r.o.s[i, 1:6] <- single.riclpm.obs.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    r.o.s[i, 7] <- traits[i]
    r.o.s[i, 8] <- "riclpm"
    r.o.s[i, 9] <- "observed"
    r.o.s[i, 10] <- "single"
}

## Combine different models
finalFit <- rbind(fit, c.l.s, r.l.s, c.o.s, r.o.s)

## Reorder columns
finalFit <- finalFit[,c("model", "type", "variables", "trait",
                        "chisq", "df", "pvalue", "cfi", "rmsea", "srmr")]

## Format table elements
finalFit$model <- toupper(finalFit$model)
finalFit$type <- str_to_title(finalFit$type)
finalFit$variables <- str_to_title(finalFit$variables)
finalFit[c(6:9,
           11:14,
           16:19,
           21:24),
         c("model",
           "type",
           "variables")]  <- NA

names(finalFit) <- c("Model",
                     "Type",
                     "Variables",
                     "Trait",
                     "ChiSq",
                     "df",
                     "p-value",
                     "CFI",
                     "RMSEA",
                     "SRMR")

papaja::apa_table(finalFit,
                  midrules=c(1,2,3,4,9,14,19),
                  digits=c(NA, NA, NA, NA, 2, 0, 3, 2, 2, 2),
                  align=rep("r", 10),
                  format.args=list(na_string=""),
                  caption="Fit indices for full sample models.",
                  note="Agr = Agreableness; Con = Conscientiousness; Ext = Extraversion; Neu = Neuroticism; Opn = Openness.")



```

Evaluating the fit of these models is challenging, as the chi-square is sensitive to sample size and subtle misfit can lead to significant values with large sample sizes, and there are no unambiguous cutoffs for well-fitting models using the other indices. Recommended cutoffs for the CFI are typically either .90 or .95, and the recommend cutoff for the RMSEA and SRMR is typically .05. The CLPM is nested within the RI-CLPM, so the difference in fit between these pairs of models can be tested explicitly, though the large sample sizes make statistical significance testing overly sensitive. Other pairs of models are not nested and cannot be directly compared. 


```{r fullPlot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Estimated Lagged Effects of Personality on Religiosity"}

################################################################################
## Collect results for full sample models
################################################################################

## Setup labels
traitLabels <- matrix(
    c(
        "agr", "a",
        "cns", "c",
        "ext", "e",
        "neu", "n",
        "opn", "o"
    ),
    nrow = 5, ncol = 2, byrow = TRUE
)


#### Functions to extract and summarize
#### For models with all traits
## Extract averages across multiple waves
extractAvg <- function(results, trait) {
    pLabel <- paste0(
        "c_r",
        trait
    )
    pLabel2 <- paste0(
        "c_",
        trait,
        "r"
    )
    ## Religion predicted from trait
    agg <- results %>%
        filter(label == pLabel) %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    ## Trait predicted from religion
    agg2 <- results %>%
        filter(label == pLabel2) %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    return(c(agg, agg2))
}

## Extract results from full set
extractParameterEstimates <- function(results,
                                      model,
                                      type,
                                      variables) {
    result <- data.frame(
        est = numeric(),
        ci.lower = numeric(),
        ci.upper = numeric(),
        est.1 = numeric(),
        ci.lower.1 = numeric(),
        ci.upper.1 = numeric(),
        trait = character(),
        model = character(),
        type = character(),
        variables = character()
    )
    for (i in 1:nrow(traitLabels)) {
        trait <- traitLabels[i, 2]
        result[i, ] <- c(
            extractAvg(results, trait),
            traitLabels[i,1],
            model,
            type,
            variables
        )
    }
    return(result)
}

#### Functions to extract results
#### These are for single-trait models
## Extract average values across waves
extractAvgSingle <- function(results) {
    ## Religion predicted from trait
    agg <- results %>%
        filter(label == "cl_t") %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    ## Trait predicted from religion
    agg2 <- results %>%
        filter(label == "cl_r") %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    return(c(agg, agg2))
}

## Extract estimates across all models
extractParameterEstimatesSingle <- function(results,
                                            model,
                                            type,
                                            variables) {
    result <- data.frame(
        est = numeric(),
        ci.lower = numeric(),
        ci.upper = numeric(),
        est.1 = numeric(),
        ci.lower.1 = numeric(),
        ci.upper.1 = numeric(),
        trait = character(),
        model = character(),
        type = character(),
        variables = character()
    )
    for (i in 1:nrow(traitLabels)) {
        trait <- traitLabels[i, 2]
        result[i, ] <- c(
            extractAvgSingle(results[[i]][[1]]),
            traitLabels[i,1],
            model,
            type,
            variables
        )
    }
    return(result)
}

location <- "testResults"
#location <- "results"

## Load results for each set of models
load(paste0(location, "/clpm.latent.results.RData"))
results.c.l.a <- clpm.latent.results[2][[1]]
c.l.a <- extractParameterEstimates(
    results.c.l.a,
    "clpm",
    "latent",
    "all"
)

load(paste0(location, "/riclpm.latent.results.RData"))
results.r.l.a <- riclpm.latent.results[2][[1]]
r.l.a <- extractParameterEstimates(
    results.r.l.a,
    "riclpm",
    "latent",
    "all"
)

load(paste0(location, "/clpm.observed.results.RData"))
results.c.o.a <- clpm.observed.results[2][[1]]
c.o.a <- extractParameterEstimates(
    results.c.o.a,
    "clpm",
    "observed",
    "all"
)

load(paste0(location, "/riclpm.observed.results.RData"))
results.r.o.a <- riclpm.observed.results[2][[1]]
r.o.a <- extractParameterEstimates(
    results.r.o.a,
    "riclpm",
    "observed",
    "all"
)

load(paste0(location, "/single.clpm.latent.results.RData"))
c.l.s <- extractParameterEstimatesSingle(
    single.clpm.latent.results,
    "clpm",
    "latent",
    "single"
)

load(paste0(location, "/single.riclpm.latent.results.RData"))
r.l.s <- extractParameterEstimatesSingle(
    single.riclpm.latent.results,
    "riclpm",
    "latent",
    "single"
)

load(paste0(location, "/single.clpm.obs.results.RData"))
c.o.s <- extractParameterEstimatesSingle(
    single.clpm.obs.results,
    "clpm",
    "observed",
    "single"
)

load(paste0(location, "/single.riclpm.obs.results.RData"))
r.o.s <- extractParameterEstimatesSingle(
    single.riclpm.obs.results,
    "riclpm",
    "observed",
    "single"
)



## Collect data from all models
plotData <- rbind(
    c.l.a,
    r.l.a,
    c.o.a,
    r.o.a,
    c.l.s,
    r.l.s,
    c.o.s,
    r.o.s
)


plotData %>%
    ggplot(
        aes(
            x = trait,
            y = est,
            ymin = ci.lower,
            ymax = ci.upper,
            color = model,
            linetype = type,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size=2) +
        geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
        coord_flip() +
        theme_bw() +
        scale_color_grey() +
        theme(
            panel.grid.major.y = element_blank(),
            panel.grid.minor = element_blank()
        ) +
    scale_y_continuous(limits = c(-.08, .08))



```

Consistent with the original paper, our focus is on the cross-lagged paths from each trait to religiosity and from religiosity to each trait. Figure \@ref(fig:fullPlot) presents the estimated cross-lagged path from each personality trait to religiosity for each of the eight tested models. Results from the CLPM are presented in black lines, whereas results for the RI-CLPM are presented as grey lines. Results from the latent-variable models are included as solid lines whereas results for the observed-variable models are presented as dashed lines. Finally results for models that include all five traits simultaneously are labeled with triangles, whereas those that model each trait individually are labeled with circles. Bars represent 95% confidence intervals for the standardized parameter estimates. 

Before describing the results, it is important to acknowledge that the confidence intervals for estimates from the RI-CLPM models are generally considerably larger than those for the CLPM. This pattern is not unique to these data or these model specifications. The RI-CLPM is a multilevel model that separates between-person associations from within-person associations, and estimates of the within-person parts of these models often have less bias at the cost of reduced efficiency [see @allison2009fixed, for an explanation]. Because of this, we highlight both the significance of the effect and the parameter estimates when comparing results across models. 

The figure shows that conclusions about the lagged effects of personality on religiosity would differ depending on which model was used. Indeed, there is no effect that emerges consistently across all model specifications. For instance, the largest effect from @entringer_big_2023 was the lagged association between agreeableness and religiosity, which had an average standardized effect of .039. The comparable model from Figure \@ref(fig:fullPlot) is the CLPM with latent traits and all traits modeled simultaneously, which resulted in an almost identical average standardized effect of `r plotData %>% filter(trait == "agr", model == "clpm", type == "latent", variables == "all") %>% select(est) %>% round(3)` This effect was actually slightly larger when the RI-CLPM was used, when latent variables were modeled, and when all traits were included simultaneously. However, estimated effects were smaller (frequently about half the size) and sometimes nonsignificant in the other model specifications. This association between agreeableness and religiosity was the most robust of the effects we examined, and even it varied across model specifications. 

Importantly, Figure \@ref(fig:fullPlot) shows that our alternative models do not always result in reduced effect sizes relative to what was reported in @entringer_big_2023. For instance, in the original study, the lagged effect of neuroticism on religiosity was a nonsignificant .011. In our reanalysis, we again found similar average estimate of `r plotData %>% filter(trait == "neu", model == "clpm", type == "latent", variables == "all") %>% select(est) %>% round(3)` with the same model specification in the full sample. However, the size and significance of the effect varied considerably across specifications. Most notably, the RI-CLPM with latent variables and all variables entered simultaneously resulted in an estimated lagged effect that was approximately the same size as the lagged effect of agreeableness (the largest effect found in the original study and a primary finding that was highlighted in the discussion). It is sometimes claimed that using the RI-CLPM underestimates causal effects [e.g., @asendorpf_modeling_2021], but this is not correct. @lucas_why_2023 showed that if stable-trait variance exists in the measures, results from the CLPM can underestimate the true lagged effects. Neuroticism received little attention in the original report, even though the lagged path from neuroticism to religiosity is one of the largest effects found when the arguably more appropriate RI-CLPM is used. 

Finally, Figure \@ref(fig:fullPlot) shows that effects can even reverse direction depending on the chosen model specification. Most notably, although there was no significant effect of conscientiousness in the original study (a finding replicated in the full sample using the same model specification), this effect became significantly negative in one version of the RI-CLPM and significantly positive in the other three specifications of the CLPM. These examples show that conclusions about the lagged associations between personality traits and religiosity depend on the precise model specification. Indeed, conclusions about all five traits depend on which model specification is chosen (if the significance of the effect is used to guide interpretation), with the sign of the effect even changing for one of the five traits.

@entringer_big_2023 found only one significant lagged association from religiosity to personality: the path from religiosity to agreeableness, which was estimated to be a small but significant .011. This effect was again replicated in our full-sample analysis (average standardized estimate = `r plotData %>% filter(trait == "agr", model == "clpm", type == "latent", variables == "all") %>% select(est.1) %>% round(3)`). However, the size and significance of this effect again varied considerably; two specifications of the CLPM resulted in considerably larger effects (with effect sizes comparable to the largest effects of personality on religion) and nonsignificantly negative associations in all specifications of the RI-CLPM. Again, when relying on the significance of the effect to guide conclusions, these would differ for three of the five traits assessed, and for two of these, the differences in the size of the estimates were considerable. 

```{r fullPlot2, echo=FALSE, message=FALSE, warning=FALSE}

plotData %>%
    ggplot(
        aes(
            x = trait,
            y = est.1,
            ymin = ci.lower.1,
            ymax = ci.upper.1,
            color = model,
            linetype = type,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size=2) +
        geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
        coord_flip() +
        theme_bw() +
        scale_color_grey() +
        theme(
            panel.grid.major.y = element_blank(),
            panel.grid.minor = element_blank()
        ) +
    scale_y_continuous(limits = c(-.08, .08))



```


# Disclosures

## Author Contributions

Richard E. Lucas conceptualized the study and wrote the initial analysis code and the first draft of the paper. 

Julia Rohrer wrote additional code and ran all analyses, contributed additional ideas for analyses, and contributed to writing and editing the text. 

## Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article. 

## Prior Versions

A preprint of this paper was posted on the PsyArXiv preprint server:  .


\newpage

# References

```{r create_r-references}
papaja::r_refs(file = "r-references.bib", append = FALSE)
```

