---
title: "On the Robustness of Reciprocal Associations Between Personality and Religiosity in a German Sample"
short title: "Personality and Religion"
author: 
  - name: Richard E. Lucas
    affiliation: 1
    corresponding: yes    # Define only one corresponding author
    address: "316 Physics Rd., Michigan State University, East Lansing, MI 48823"
    email: "lucasri@msu.edu"
  - name: Julia Rohrer
    affiliation: 2
affiliation:
  - id: 1
    institution: "Department of Psychology, Michigan State University"
  - id: 2
    institution: "Department of Psychology, University of Leipzig"


abstract: |
  Here's an abstract. 
  
  
keywords: "personality, religiosity, cross-lagged panel model"

wordcount: 

header-includes:
   - \usepackage{todonotes}
   - \usepackage{setspace}
   - \AtBeginEnvironment{tabular}{\singlespacing}
   - \AtBeginEnvironment{lltable}{\singlespacing}
   - \AtBeginEnvironment{ThreePartTable}{\singlespacing}
   - \AtBeginEnvironment{tablenotes}{\doublespacing}
   - \captionsetup[table]{font={stretch=1.5}}
   - \captionsetup[figure]{font={stretch=1.5}}
   - \raggedbottom

bibliography:
   - '/home/rich/Dropbox/MyLibraryZ2.bib'
   - r-references.bib
floatsintext: yes
mask: no
linenumbers: no
documentclass: "apa6"
classoption: "man"
output: 
  papaja::apa6_pdf:
  fig_caption: yes

---

```{r setup, include=FALSE}
## Load packages
library(lavaan)
library(tidyverse)
library(knitr)
library(papaja)
library(directlabels)
library(gridExtra)
library(ggplot2)
library(stringr)

## Set options
options(knitr.kable.NA='')

## Change this for final version
location <- "testResults"

```


A common goal in personality research is to identify robust associations between personality characteristics and consequential behaviors and outcomes. Identifying these associations allows researchers to develop and test hypotheses that inform personality theories. For instance, researchers may study the links between a trait like conscientiousness and an outcome like job achievement. A greater understanding of the processes underlying this association can inform theories about how conscientiousness affects outcomes in the real world. In addition, this research could provide practical guidance for those seeking to improve achievement levels. Moreover, a consideration of the reverse causal direction---understanding whether achievement experiences impact trait levels---can inform theories of personality development and change. Thus, studies that examine the processes underlying such links have great potential further the understanding of how personality characteristics shape peoples lives, and how life experiences shape personality. 

Recently, @entringer_big_2023 conducted such an examination, investigating the links between the Big Five personality traits and religiosity in a very large German sample. This research was motivated both by prior theories meant to explain how personality can shape religiosity and by theories that posit that religiosity can affect personality. For instance, a niche-picking perspective suggests that people who have personality traits that are consistent with the behaviors that are typically exhibited in religious contexts should gravitate towards these religious contexts. In addition, the Sociocultural Norm Perspective (Eck & Gebauer, 2022) posits that personality traits like agreeableness, conscientiousness, and (low) openness to experience produce normative behaviors; when religiosity is normative in a culture, then these traits should cause greater religiosity. In contrast, complementary theories suggest that religiosity itself can impact these same traits, as religious contexts also promote or even enforce behaviors and views that are consistent with traits like agreeableness and conscientiousness. 

To test these ideas, @entringer_big_2023 relied on a widely used approach for examining reciprocal causal effects in panel data: the cross-lagged panel model [CLPM, @heise_causal_1970]. In the CLPM, each variable (in this case, personality and religiosity) at each occasion is predicted from the same variables assessed at prior waves. With some assumptions, lagged associations from one variable to the other (e.g., from Time 1 personality to Time 2 religiosity) can be interpreted as causal effects. @entringer_big_2023 found that agreeableness, openness, and conscientiousness prospectively predicted changes in religiosity (at least in some contexts), whereas religiosity predicted changes in agreeableness and openness (again, in some contexts). Moreover, the religiosity of the region in which respondents lived moderated the links between some personality traits and religion. 

Entringer et al.'s [-@entringer_big_2023] study had a number of desirable features that make it especially well-suited to examining questions about reciprocal associations between personality and religiosity. First, the authors used a very large panel study with four waves of assessment over a period of twelve years. These features should contribute to the robustness of the results. Moreover, the authors used a sophisticated latent-variable version of the CLPM that accounts for measurement error, which can help reduce the likelihood of spurious lagged associations [@lucas_why_2023]. In addition, the authors examined these associations separately in different German federal states that varied in overall religiosity, which allowed them to examine theoretically relevant contextual moderators of these associations. Finally, the authors conducted many robustness checks to support their primary findings. 

## Concerns About Robustness

Despite these strengths, however, there are reasons to be concerned about the robustness of the support for reciprocal causal effects between personality and religiosity. The first issue concerns the size of the effects that @entringer_big_2023 found. The authors foreshadow this concern early in the paper, noting that lagged effects are typically much smaller than cross-sectional effects and that prior cross-sectional correlations should provide an upper bound on the size of any lagged associations. Because the correlations between personality and religiosity tend to be small (e.g., around .19 in their review), the lagged effects should be even smaller. Indeed, the observed lagged effects in their study were quite small, with maximum standardized regression coefficients of .039. The estimated moderator effects they found were similarly small in size.

Although we agree that small effects can sometimes be important, such effects provide challenges for interpretation. First, just because such effects can be important in some contexts does not mean that they are always important; justification for why a particular small effect is important is needed. Moreover, one common defense of the importance of small effects is that these effects accumulate over time. However, this should mean that aggregated between-person correlations should themselves be reasonably large in size. @entringer_big_2023 did not report zero-order correlations either within waves or after aggregating personality and religiosity across the 12-year period. 

Perhaps more importantly, very small effect sizes can be problematic because subtle model misspecification or residual confounding can lead to small effects. This makes it difficult to distinguish true effects from model misspecification or confounding when effects are small. Additional concerns about model misspecification and confounding could easily lead to the size of effects found in this study. 

For instance, although Entringer et al.'s decision to model latent personality factors when examining reciprocal associations has the desirable feature of removing measurement error, it also comes with a cost in terms of model complexity. Although researchers might hope that the items of their measures load cleanly on the factor to which the item belongs (and not to any other), this is not always the case in practice. In such cases, allowing for secondary loadings may be necessary to improve model fit, though questions can remain about whether such post hoc modifications capitalize on chance. If decisions about the measurement model affect the structural features of the model, then concerns about the robustness of estimates (especially estimates that are very small in size) can be raised. In the current study, we compare the latent-variable model from @entringer_big_2023 to a simpler observed-variable model that includes only the observed mean scores for each personality trait measure. 

A second concern is that @entringer_big_2023 chose to model all five traits simultaneously when predicting changes in religiosity. Although the Big Five traits are hypothesized to be relatively independent, in practice they are not. Thus, when modeling all traits simultaneously, estimated paths from personality to religiosity reflect associations that persist after controlling for all other personality traits. The decision to control for correlated variables comes with interpretational challenges, however, as the association can only be interpreted as an association between religiosity and the variance that is not shared with other traits [@lynam_perils_2006]. The conceptual connections between this residualized variable and the hypothetical construct it is meant to assess are not always clear. Although @entringer_big_2023 justified their decision to simultaneously model all five traits by noting that this decision is consistent with prior research, a more substantive justification would be preferable. Because of the interpretational challenges, our preference is to interpret unadjusted associations; but at the very least, robustness across modeling choices is important to consider. 

The final concern is potentially the most consequential. When examining reciprocal effects, @entringer_big_2023 relied solely on the traditional CLPM, which includes a single lagged association with the prior wave. However, @hamaker_critique_2015 showed that the CLPM results in biased lagged associations when stable-trait variance exists in the measures being modeled. Recently, @lucas_why_2023 used simulations to show that this problem is quite severe; spurious lagged associations can be found as often as 100% of the time in realistic scenarios (e.g., when there is just a moderate amount of stable-trait variance, when sample sizes are moderate to large, and when multiple waves of assessment are included)[^orth]. The size of the bias found in these simulations is high relative to the size of effects reported by @entringer_big_2023. Notably, two strengths of Entringer et al.'s study (the very large sample size and the use of a four-wave design) increase the likelihood of finding spurious lagged effects. 

[^orth]: Recently, @orth_testing_2021 defended the CLPM against concerns raised by @hamaker_critique_2015 and others. Their defense focused on the interpretation of the between-person and within-person associations that are modeled in the alternatives to the CLPM. Specifically, they argued that the CLPM can test "between-person prospective effects," while the alternatives to the CLPM do not. As explained in @lucas_why_2023, we disagree that the between-person prospective effects that Orth et al. hope to assess are clearly defined, and we disagree that the CLPM tests them [see @lucas_why_2023 for a discussion]. More importantly, this defense ignores the fact that a critical assumption of the CLPM is that no additional source of stability in the outcome measures exists beyond those that are included in the model (i.e., the autoregressive effect reflected in the stability of a variable over time and the lagged effect of one variable on the other at a later time). If stable-trait variance exists, then this assumption is violated, invalidating the interpretation of the lagged paths as causal effects [@heise_causal_1970], regardless of whether those effects are described as being between-person or within-person effects. 

## The Present Study

Although @entringer_big_2023 took many steps to ensure the robustness of their results, additional concerns can be raised about effect sizes, model complexity, the decision to simultaneously model all five personality traits, and---most importantly---the decision to use a traditional CLPM instead of a model that accounts for additional sources of stability. The goal of the current analysis is to test the robustness of these results to alternative specifications. 

We first simply examine the correlations between religiosity and each of the Big Five traits when each is aggregated across all waves. This provides a simple index of effect size that helps establish whether any lagged causal effects accumulate over time. Next, we test a series of models that examine the robustness of the results reported in @entringer_big_2023. Specifically, after first replicating their results, we then test various combinations of three modifications. We compare models where the Big Five traits are modeled as latent-traits (using the same measurement model in the original paper, including secondary loadings) versus when they are modeled as observed variables. We compare models where the Big Five traits are entered simultaneously as predictors to those where separate models are run for each trait individually. Finally, we compare the CLPM to the random-intercept cross-lagged panel model [RI-CLPM, @hamaker_critique_2015], which includes a random intercept to account for stable-trait variance. Combining each pair of comparisons results in eight separate models, with results for each of the Big Five traits. In addition, @entringer_big_2023 examined the moderating contextual effect of state-level religiosity. We also test these moderating effects in each of the eight models for each of the Big Five traits. The results will then be compared for robustness.

# Methods

This paper uses data from the German Socio-Economic Panel (SOEP) study, which assessed the Big Five personality traits and religiosity four times, at four-year intervals. The inclusion of four waves of assessment allows for the use of both the CLPM and the more complex RI-CLPM. @entringer_big_2023 provided detailed code for their analyses, however, they did not provide code to extract and clean data from the raw data files. As a test of the reproducibility of the analyses from the description provided in the text, we developed our own code for extracting and cleaning data. Despite our best attempts, the exact number of participants included in the final samples, and the precise estimates from the original model could not be perfectly reproduced. However, both the final sample sizes and results are quite close to the original, and all substantive conclusions from the original paper were supported in analyses that use the same model. Thus we proceeded with our robustness checks using the sample we extracted. Our full code for extracting, cleaning, and analyzing these data is available at [https://github.com/rlucas11/gsoep_religion](https://github.com/rlucas11/gsoep_religion)). 

## Participants

The SOEP is a long-running panel study 


# Results

We first examine the size of the correlations between each Big Five personality trait and religiosity. To allow any short-term associations to accumulate over time, we focus on the correlations between scores aggregated across all available assessments. Specifically, we computed each person's average score for each personality trait across all available waves and the average religiosity score across all waves. We then examined the correlations among these variables (a) in each state, (b) by pooling the within-state associations, and (c) by examining the raw correlation among all participants, ignoring residency. These correlations are reported in table \@ref(tab:correlations). 


```{r correlations, echo=FALSE, message=FALSE, warning=FALSE}

################################################################################
## Correlations by state
################################################################################
load("results/correlationsByState.RData")
states <- read_csv("info/states.csv")

## Initialize data frame
corTab <- data.frame(
    State = numeric(),
    Agr = numeric(),
    Con = numeric(),
    Ext = numeric(),
    Neu = numeric(),
    Opn = numeric(),
    Religiosity = numeric(),
    N = numeric()
)

## Extract results
for (i in 1:16) {
    corTab[i, ] <- c(
        states[i, 2],
        out$r[i][[1]][6, 1:5],
        out$mean[i, 6],
        out$n[i, 7]
    )
}

corTab[17,1] <- "Pooled Within"
corTab[17,2:6] <- out$rwg[6, 1:5]
corTab[17,8] <- sum(corTab[1:16, 8])
corTab[18,1] <- "Raw Correlation"
corTab[18,2:6] <- out$raw[6, 1:5]
corTab[18,8] <- sum(corTab[1:16, 8])

corTab$N <- as.character(corTab$N)


papaja::apa_table(corTab,
                  midrules=c(16),
                  align=rep("r", 8),
                  format.args=list(na_string=""),
                  col_spanners=list(`Correlation with Religiosity`=c(2, 6)),
                  caption="Within-state correlations between each personality trait and religiosity.",
                  note="Agr = Agreableness; Con = Conscientiousness; Ext = Extraversion; Neu = Neuroticism; Opn = Openness. Mean religiosity and sample sizes are presented in the rightmost columns.")


```

As can be seen in this table, even after aggregating over a 12-year period, all correlations are quite small. The largest correlation for the full sample is between religiosity and agreeableness, which is just `r corTab[18,2]`. The largest correlation among the `r 5*16` tested (including those from the full sample and pooled within-state associations) was just `r max(corTab[1:18, 2:6])`. Thus, even when allowing an effect to accumulate over a 12-year period, correlations between personality and religiosity are quite small. We next turn to models that examine the reciprocal associations between personality and religiosity. 

## Full Sample Analyses

In their original paper, @entringer_big_2023 focused on results from each federal state individually and on the meta-analytic averages of these state-level associations. This allowed them to examine whether results varied across states and to test whether state-level religion moderated associations between religiosity and personality. Although we also use this approach for some of our analyses, we first present model comparisons in the full sample, ignoring state of residence. We do this for three reasons. First, as we will show, the estimates from the full sample mostly replicate the meta-analytic averages across the 14 analyzed states, and focusing on one sample instead of 14 (or 16) allows for clearer model comparisons. Second (and most importantly), in the original analyses, two states were dropped because of small sample sizes that resulted in problems with model convergence. When testing additional models, the specific states in which estimation problems emerge differ across models, which means that any overall differences that result could be due to differences in the models or differences in which states provide estimates. Finally, by ignoring the state-level structure, no participants needed to be excluded due to residence in states for which sample sizes were small. Again, we note that we eventually do discuss results in each state individually in the next section. 

The models we tested compare estimates across three dichotomous modeling decisions: (1) The original CLPM versus the potentially more appropriate RI-CLPM, (2) Models that include latent personality traits (with a complex measurement model) versus models that include only a single observed mean score for each trait, and (c) models that include all five traits simultaneously (and hence control for correlations among traits when predicting religiosity) versus models that include just one trait per model. The combination of these factors results in eight sets of estimates for each personality trait. 

```{r fullFit, echo=FALSE, message=FALSE, warning=FALSE}

## Change this for final paper
location <- "testResults"

## Initialize df
fit <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

## Load results for each set of models
load(paste0(location, "/clpm.latent.results.RData"))
fit[1, 1:6] <- clpm.latent.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[1, 7:10] <- c(
        NA,
        "clpm",
        "latent",
        "all")

load(paste0(location, "/riclpm.latent.results.RData"))
fit[2, 1:6] <- riclpm.latent.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[2, 7:10] <- c(
    NA,
    "riclpm",
    "latent",
    "all"
)


load(paste0(location, "/clpm.observed.results.RData"))
fit[3, 1:6] <- clpm.observed.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[3, 7:10] <- c(
    NA,
    "clpm",
    "observed",
    "all"
)

load(paste0(location, "/riclpm.observed.results.RData"))
fit[4, 1:6] <- riclpm.observed.results[[1]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
fit[4, 7:10] <- c(
    NA,
    "riclpm",
    "observed",
    "all"
)

## List traits for loop in single-trait models
traits <- c("agr", "cns", "ext", "neu", "opn")

load(paste0(location, "/single.clpm.latent.results.RData"))
c.l.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    c.l.s[i, 1:6] <- single.clpm.latent.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    c.l.s[i, 7] <- traits[i]
    c.l.s[i, 8] <- "clpm"
    c.l.s[i, 9] <- "latent"
    c.l.s[i, 10] <- "single"
}
    

load(paste0(location, "/single.riclpm.latent.results.RData"))
r.l.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    r.l.s[i, 1:6] <- single.riclpm.latent.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    r.l.s[i, 7] <- traits[i]
    r.l.s[i, 8] <- "riclpm"
    r.l.s[i, 9] <- "latent"
    r.l.s[i, 10] <- "single"
}


load(paste0(location, "/single.clpm.obs.results.RData"))
c.o.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    c.o.s[i, 1:6] <- single.clpm.obs.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    c.o.s[i, 7] <- traits[i]
    c.o.s[i, 8] <- "clpm"
    c.o.s[i, 9] <- "observed"
    c.o.s[i, 10] <- "single"
}


load(paste0(location, "/single.riclpm.obs.results.RData"))
r.o.s <- data.frame(
    chisq = numeric(),
    df = numeric(),
    pvalue = numeric(),
    cfi = numeric(),
    rmsea = numeric(),
    srmr = numeric(),
    trait = character(),
    model = character(),
    type = character(),
    variables = character()
)

for (i in 1:5) {
    r.o.s[i, 1:6] <- single.riclpm.obs.results[[i]][[2]][c(
        "chisq",
        "df",
        "pvalue",
        "cfi",
        "rmsea",
        "srmr"
        )]
    r.o.s[i, 7] <- traits[i]
    r.o.s[i, 8] <- "riclpm"
    r.o.s[i, 9] <- "observed"
    r.o.s[i, 10] <- "single"
}

## Combine different models
finalFit <- rbind(fit, c.l.s, r.l.s, c.o.s, r.o.s)

## Reorder columns
finalFit <- finalFit[,c("model", "type", "variables", "trait",
                        "chisq", "df", "pvalue", "cfi", "rmsea", "srmr")]

## Format table elements
finalFit$model <- toupper(finalFit$model)
finalFit$type <- str_to_title(finalFit$type)
finalFit$variables <- str_to_title(finalFit$variables)
finalFit[c(6:9,
           11:14,
           16:19,
           21:24),
         c("model",
           "type",
           "variables")]  <- NA

names(finalFit) <- c("Model",
                     "Type",
                     "Variables",
                     "Trait",
                     "ChiSq",
                     "df",
                     "p-value",
                     "CFI",
                     "RMSEA",
                     "SRMR")

papaja::apa_table(finalFit,
                  midrules=c(1,2,3,4,9,14,19),
                  digits=c(NA, NA, NA, NA, 2, 0, 3, 2, 2, 2),
                  align=rep("r", 10),
                  format.args=list(na_string=""),
                  caption="Fit indices for full sample models.",
                  note="Agr = Agreableness; Con = Conscientiousness; Ext = Extraversion; Neu = Neuroticism; Opn = Openness.")



```

Fit indices for all models are presented in Table \@ref(tab:fullFit). Consistent with @entringer_big_2023, we report chi-square with degrees of freedom and p-value, along with CFI, RMSEA, and SRMR. Evaluating the fit of these models is challenging, as the chi-square is sensitive to sample size and subtle misfit can lead to significant values with large sample sizes, and there are no unambiguous cutoffs for well-fitting models using the other indices. Recommended cutoffs for the CFI are typically either .90 or .95, and the recommend cutoff for the RMSEA and SRMR is typically .05. @entringer_big_2023 considered models with CFI values close to (and sometimes below) .90 as acceptable, but we generally prefer the .95 cutoff. It is important to note that the CLPM is nested within the RI-CLPM, so the difference in fit between these pairs of models can be tested explicitly, though the large sample sizes make statistical significance testing overly sensitive. Other pairs of models are not nested and cannot be directly compared. 

```{r measurement-fit, echo=FALSE, message=FALSE, warning=FALSE}
## Get fit data for measurement model; referenced in text
load(paste0(location, "/measurement.results.RData"))

```

### Model Fit

Table \@ref(tab:fullFit) shows that these modeling decisions affect fit. Although RMSEA and SRMR values for the original model are acceptable, the CFI for the CLPM with latent variables and all traits included just clears the .90 criterion and does not reach the more stringent .95 threshold. These results are consistent with those from the original paper. Indeed, in the original paper, the CFI for the primary model was below the cutoff of .90 in 13 of 14 states. These fit indices could be cause for concern when interpreting the estimates from the models. 

Notably, the RI-CLPM with latent variables and all traits included (shown in the second row of Table \@ref(tab:fullFit) also has a CFI value below .95 (even though the Chi-Square value is considerably---and significantly---lower for this model than for the CLPM). In fact, even the CFI for a simple measurement model does not exceed the more stringent .95 cutoff, with a value of `r round(measurement.results[[2]]["cfi"], 2)`. This is also consistent with the results in the original paper, where the CFI for a measurement model specifying metric invariance across states had a CFI value of just .92. The fact that the fit of this unconstrained measurement model is not especially strong provides further justification to examine simpler models with less complicated measurement models. Misspecification in this part of the model could affect estimates of the structural parts. 

Moving to the comparison of the CLPM and the RI-CLPM for the model with observed trait measures, the difference in fit indices is even clearer. In this case, the CFI for the CLPM is still a borderline acceptable `r round(finalFit[3, "CFI"], 2)` whereas for the RI-CLPM it is `r round(finalFit[4, "CFI"], 2)`. Moreover, the difference in Chi-square values for the two models is not just significant, but dramatic, dropping from `r round(finalFit[3, "ChiSq"], 2)` to `r round(finalFit[4, "ChiSq"], 2)` for models that differ by just 21 degrees of freedom. 



```{r implied-cors, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Actual stability coefficients (solid line) and implied stability coefficients from the CLPM (short-dashed line) and RI-CLPM (long-dashed line)."}

################################################################################
## Compare pattern of correlations
################################################################################

load(paste0(location,
            "corsForPlot.RData")
     )



aPlot <- ggplot(
    data = corsForPlot[[1]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula = y ~ log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1)) +
        theme(legend.position = "none") +
        ggtitle("Agreeableness") +
        scale_x_continuous(breaks = c(4, 8, 12))

cPlot <- ggplot(
    data = corsForPlot[[2]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula=y~log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1))+
        theme(legend.position = "none") +
        ggtitle("Conscientiousness") +
        scale_x_continuous(breaks = c(4, 8, 12))

ePlot <- ggplot(
    data = corsForPlot[[3]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula=y~log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1))+
        theme(legend.position = "none") +
        ggtitle("Extraversion") +
        scale_x_continuous(breaks = c(4, 8, 12))

nPlot <- ggplot(
    data = corsForPlot[[4]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula=y~log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1)) +
        theme(legend.position = "none") +
        ggtitle("Neuroticism") +
        scale_x_continuous(breaks = c(4, 8, 12))

oPlot <- ggplot(
    data = corsForPlot[[5]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula=y~log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1)) +
        theme(legend.position = "none") +
        ggtitle("Openness") +
        scale_x_continuous(breaks = c(4, 8, 12))

rPlot <- ggplot(
    data = corsForPlot[[6]],
    aes(
        x = Lag,
        y = Stability,
        group = Model,
        linetype = Model
    )
) +
    geom_smooth(formula=y~log(x), span = 2, color = "black") +
        theme_bw() +
        scale_y_continuous(limits = c(0, 1))+
        theme(legend.position = "none") +
        ggtitle("Religiosity") +
        scale_x_continuous(breaks = c(4, 8, 12))


grid.arrange(aPlot, cPlot, ePlot, nPlot, oPlot, rPlot, nrow = 2)


```

It is easy to understand the source of misfit in the CLPM just by considering the implied stability coefficients from such a model and then comparing them to the actual correlations from the data. @lucas_why_2023 noted that a major problem with the CLPM is that the model implies that stability coefficients should decline quickly with increasing lags (especially when cross-lagged paths are small), yet actual stability coefficients for most variables are quite stable over increasingly long lags. This is also true in these data, as can be seen in Figure \@ref(fig:implied-cors). This figure plots actual stability coefficients for each variable across 4-, 8-, and 12-year intervals in the solid lines. Stability coefficients start out moderate for 4-year intervals, but decline only slightly across 8- and 12-year intervals. The implied stability coefficients from the CLPM (shown in the lines with short dashes), however, decline much faster than the actual stability coefficients, leading to a predicted stability of approximately half the actual stability for the 12-year intervals. In contrast, the RI-CLPM (shown in the lines with long dashes) reproduces the patterns of stability coefficients almost perfectly. The CLPM is poorly suited to describing patterns of stability, which means that the model is mis-specified. This mis-specification will then bias other parameters in the model. 

This same pattern of differences in fit indices emerges in the single-trait models, with the RI-CLPM consistently outperforming the CLPM, especially in the observed-trait models[^residuals]. In short, the fit indices suggest that the measurement model for the latent-trait models may not fit the data well. In addition, the RI-CLPM consistently fits better than the CLPM, which can be explained by the fact that the CLPM cannot account for the slow decline in stability over increasingly long lags, whereas the RI-CLPM can. The choice to model all traits together versus separately cannot be informed by fit statistics. 

[^residuals]: The fact that the difference between the CLPM and RI-CLPM is arguably not as large in the latent-variable models as in the observed-variable models highlights an interesting modeling decision by the authors that is not uncommon in latent-variable applications of the CLPM. Specifically, although the structural part of the model assumes that the autoregressive process and lagged associations include only lagged effects from the immediately prior wave, the item-specific residuals are allowed to correlate with item-specific residuals *from all other waves*. In other words, by omitting a random intercept or its equivalent, those who use the CLPM deny that there is any association between non-adjacent waves that can't be explained by the lag-1 associations, whereas the item-specific residuals have a more general, "trait-like" structure. It is not clear why expectations for the residuals would differ in this way. In any case, including these residual correlations in latent-trait models likely improves fit by accounting for at least some of the true-stable trait variance that the RI-CLPM models explicitly. However, if stable-trait variance is responsible for these associations, the model would still be mis-specified, despite the better fit. 


```{r fullPlot, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Estimated Lagged Effects of Personality on Religiosity"}

################################################################################
## Collect results for full sample models
################################################################################

## Setup labels
traitLabels <- matrix(
    c(
        "agr", "a",
        "cns", "c",
        "ext", "e",
        "neu", "n",
        "opn", "o"
    ),
    nrow = 5, ncol = 2, byrow = TRUE
)


#### Functions to extract and summarize
#### For models with all traits
## Extract averages across multiple waves
extractAvg <- function(results, trait) {
    pLabel <- paste0(
        "c_r",
        trait
    )
    pLabel2 <- paste0(
        "c_",
        trait,
        "r"
    )
    ## Religion predicted from trait
    agg <- results %>%
        filter(label == pLabel) %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    ## Trait predicted from religion
    agg2 <- results %>%
        filter(label == pLabel2) %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    return(c(agg, agg2))
}

## Extract results from full set
extractParameterEstimates <- function(results,
                                      model,
                                      type,
                                      variables) {
    result <- data.frame(
        est = numeric(),
        ci.lower = numeric(),
        ci.upper = numeric(),
        est.1 = numeric(),
        ci.lower.1 = numeric(),
        ci.upper.1 = numeric(),
        trait = character(),
        model = character(),
        type = character(),
        variables = character()
    )
    for (i in 1:nrow(traitLabels)) {
        trait <- traitLabels[i, 2]
        result[i, ] <- c(
            extractAvg(results, trait),
            traitLabels[i,1],
            model,
            type,
            variables
        )
    }
    return(result)
}

#### Functions to extract results
#### These are for single-trait models
## Extract average values across waves
extractAvgSingle <- function(results) {
    ## Religion predicted from trait
    agg <- results %>%
        filter(label == "cl_t") %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    ## Trait predicted from religion
    agg2 <- results %>%
        filter(label == "cl_r") %>%
        select(est.std, ci.lower, ci.upper) %>%
        summarise(
            est = mean(est.std),
            ci.lower = mean(ci.lower),
            ci.upper = mean(ci.upper)
        )
    return(c(agg, agg2))
}

## Extract estimates across all models
extractParameterEstimatesSingle <- function(results,
                                            model,
                                            type,
                                            variables) {
    result <- data.frame(
        est = numeric(),
        ci.lower = numeric(),
        ci.upper = numeric(),
        est.1 = numeric(),
        ci.lower.1 = numeric(),
        ci.upper.1 = numeric(),
        trait = character(),
        model = character(),
        type = character(),
        variables = character()
    )
    for (i in 1:nrow(traitLabels)) {
        trait <- traitLabels[i, 2]
        result[i, ] <- c(
            extractAvgSingle(results[[i]][[1]]),
            traitLabels[i,1],
            model,
            type,
            variables
        )
    }
    return(result)
}

location <- "testResults"
#location <- "results"

## Load results for each set of models
load(paste0(location, "/clpm.latent.results.RData"))
results.c.l.a <- clpm.latent.results[2][[1]]
c.l.a <- extractParameterEstimates(
    results.c.l.a,
    "clpm",
    "latent",
    "all"
)

load(paste0(location, "/riclpm.latent.results.RData"))
results.r.l.a <- riclpm.latent.results[2][[1]]
r.l.a <- extractParameterEstimates(
    results.r.l.a,
    "riclpm",
    "latent",
    "all"
)

load(paste0(location, "/clpm.observed.results.RData"))
results.c.o.a <- clpm.observed.results[2][[1]]
c.o.a <- extractParameterEstimates(
    results.c.o.a,
    "clpm",
    "observed",
    "all"
)

load(paste0(location, "/riclpm.observed.results.RData"))
results.r.o.a <- riclpm.observed.results[2][[1]]
r.o.a <- extractParameterEstimates(
    results.r.o.a,
    "riclpm",
    "observed",
    "all"
)

load(paste0(location, "/single.clpm.latent.results.RData"))
c.l.s <- extractParameterEstimatesSingle(
    single.clpm.latent.results,
    "clpm",
    "latent",
    "single"
)

load(paste0(location, "/single.riclpm.latent.results.RData"))
r.l.s <- extractParameterEstimatesSingle(
    single.riclpm.latent.results,
    "riclpm",
    "latent",
    "single"
)

load(paste0(location, "/single.clpm.obs.results.RData"))
c.o.s <- extractParameterEstimatesSingle(
    single.clpm.obs.results,
    "clpm",
    "observed",
    "single"
)

load(paste0(location, "/single.riclpm.obs.results.RData"))
r.o.s <- extractParameterEstimatesSingle(
    single.riclpm.obs.results,
    "riclpm",
    "observed",
    "single"
)



## Collect data from all models
plotData <- rbind(
    c.l.a,
    r.l.a,
    c.o.a,
    r.o.a,
    c.l.s,
    r.l.s,
    c.o.s,
    r.o.s
)


plotData %>%
    ggplot(
        aes(
            x = trait,
            y = est,
            ymin = ci.lower,
            ymax = ci.upper,
            color = model,
            linetype = type,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size=2) +
        geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
        coord_flip() +
        theme_bw() +
        scale_color_grey() +
        theme(
            panel.grid.major.y = element_blank(),
            panel.grid.minor = element_blank()
        ) +
    scale_y_continuous(limits = c(-.08, .08))



```

### Estimates of Lagged Associations

Consistent with the original paper, our focus is on the cross-lagged paths from each trait to religiosity and from religiosity to each trait. Figure \@ref(fig:fullPlot) presents the estimated cross-lagged path from each personality trait to religiosity for each of the eight tested models. Results from the CLPM are presented in black lines, whereas results for the RI-CLPM are presented as grey lines. Results from the latent-variable models are included as solid lines whereas results for the observed-variable models are presented as dashed lines. Finally results for models that include all five traits simultaneously are labeled with triangles, whereas those that model each trait individually are labeled with circles. Bars represent 95% confidence intervals for the standardized parameter estimates. 

Before describing the results, it is important to acknowledge that the confidence intervals for estimates from the RI-CLPM models are generally considerably larger than those for the CLPM. This pattern is not unique to these data or these model specifications. The RI-CLPM is a multilevel model that separates between-person associations from within-person associations, and estimates of the within-person parts of these models often have less bias at the cost of reduced efficiency [see @allison2009fixed, for an explanation]. Because of this, we highlight both the significance of the effect and the parameter estimates when comparing results across models. 

The figure shows that conclusions about the lagged effects of personality on religiosity would differ depending on which model was used. Indeed, there is no effect that emerges consistently across all model specifications. For instance, the largest effect from @entringer_big_2023 was the lagged association between agreeableness and religiosity, which had an average standardized effect of .039. The comparable model from Figure \@ref(fig:fullPlot) is the CLPM with latent traits and all traits modeled simultaneously, which resulted in an almost identical average standardized effect of `r plotData %>% filter(trait == "agr", model == "clpm", type == "latent", variables == "all") %>% select(est) %>% round(3)` This effect was actually slightly larger when the RI-CLPM was used, when latent variables were modeled, and when all traits were included simultaneously. However, estimated effects were smaller (frequently about half the size) and sometimes nonsignificant in the other model specifications. This association between agreeableness and religiosity was the most robust of the effects we examined, and even it varied across model specifications. 

Importantly, Figure \@ref(fig:fullPlot) shows that our alternative models do not always result in reduced effect sizes relative to what was reported in @entringer_big_2023. For instance, in the original study, the lagged effect of neuroticism on religiosity was a nonsignificant .011. In our reanalysis, we again found similar average estimate of `r plotData %>% filter(trait == "neu", model == "clpm", type == "latent", variables == "all") %>% select(est) %>% round(3)` with the same model specification in the full sample. However, the size and significance of the effect varied considerably across specifications. Most notably, the RI-CLPM with latent variables and all variables entered simultaneously resulted in an estimated lagged effect that was approximately the same size as the lagged effect of agreeableness (the largest effect found in the original study and a primary finding that was highlighted in the discussion). It is sometimes claimed that using the RI-CLPM necessarily underestimates causal effects relative to estimates from the CLPM [e.g., @asendorpf_modeling_2021], but this is not correct. @lucas_why_2023 showed that if stable-trait variance exists in the measures, results from the CLPM can underestimate the true lagged effects. Neuroticism received little attention in Entringer et al.'s [-@entringer_big_2023] original report, even though the lagged path from neuroticism to religiosity is one of the largest effects found when the arguably more appropriate RI-CLPM is used. 

Finally, Figure \@ref(fig:fullPlot) shows that effects can even reverse direction depending on the chosen model specification. Most notably, although there was no significant effect of conscientiousness in the original study (a finding replicated in the full sample using the same model specification), this effect became significantly negative in one version of the RI-CLPM and significantly positive in the other three specifications of the CLPM. These examples show that conclusions about the lagged associations between personality traits and religiosity depend on the precise model specification. Indeed, conclusions about all five traits depend on which model specification is chosen (if the significance of the effect is used to guide interpretation), with the sign of the effect even changing for one of the five traits.

```{r fullPlot2, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Estimated Lagged Effects of Religiosity on Personality"}

plotData %>%
    ggplot(
        aes(
            x = trait,
            y = est.1,
            ymin = ci.lower.1,
            ymax = ci.upper.1,
            color = model,
            linetype = type,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size=2) +
        geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
        coord_flip() +
        theme_bw() +
        scale_color_grey() +
        theme(
            panel.grid.major.y = element_blank(),
            panel.grid.minor = element_blank()
        ) +
    scale_y_continuous(limits = c(-.08, .08))



```

Turning to the lagged paths from religiosity to personality, @entringer_big_2023 found only one significant effect: the path from religiosity to agreeableness, which was estimated to be a small but significant .011. The size and significance of this effect was again replicated in our full-sample analysis (average standardized estimate = `r plotData %>% filter(trait == "agr", model == "clpm", type == "latent", variables == "all") %>% select(est.1) %>% round(3)`). However, as Figure \@ref(fig:fullPlot2) shows, the size and significance of this lagged effect again varied considerably; two specifications of the CLPM resulted in considerably larger effects for the path from religiosity to agreeableness (with effect sizes comparable to the largest effects of personality on religion) and nonsignificantly negative associations in all specifications of the RI-CLPM. Again, when relying on the significance of the effect to guide conclusions, these would differ for three of the five traits assessed, and for two of these, the differences in the size of the estimates were considerable. 

In addition, estimates for three of the four other effects differed depending on which model specification was used. For instance, the lagged effects of religiosity on openness and conscientiousness, effects that were not significant in the original paper were significant in both the observed-variable models. Moreover, the effect of agreeableness was much stronger in the observed variable models than in the original model (though in all versions of the CLPM, this effect was significant). There was also one effect that was significant in the full sample that was not significant when using the same model in the original study: the lagged effect of religiosity on extraversion. The effect was a non-significant .004 in the original paper, whereas it was a significant, though only slightly larger `r plotData %>% filter(trait == "ext", model == "clpm", type == "latent", variables == "all") %>% select(est.1) %>% round(3)` in the current analyses. Again, as was true with the paths from personality to religiosity, conclusions about the paths from religiosity to personality vary depending on which model specification one chooses. 

## Meta-analysis Across Federal States

As noted previously, the results from the full-sample analyses mostly replicate the average effects from the meta-analysis of results across the 14 federal states analyzed in @entringer_big_2023. By focusing on the full sample, comparisons across models were simplified, and the effect of any state-specific model estimation problems could be eliminated. However, using this full-sample approach, it is not possible to replicate the state-level contextual moderator analyses, so we also conducted state-level analyses to examine how modeling choices affect state-level religiosity as a moderator of the lagged effects. 

Assessing how model choice affects results across states is challenging, however, as estimation and convergence problems occur at different rates across different models. Even in the original paper, responses from two states (Bremen and Saarland) were dropped from the analyses due to estimation problems in the CLPM. Our own analyses replicated these problems in these two states, as well as in one additional state not identified in the original analyses. Specifically, although estimates could be obtained for Hamburg, some variances were estimated to be negative. Additional problems (including negative variances, non-positive-definite matrices, and lack of convergence) were encountered with other model specifications, including the CLPM with latent variables and each trait modeled separately and the RI-CLPM with latent variables traits modeled separately or together. Notably, no estimation or convergence problems were encountered with any model (even those in the two states that were omitted from the original paper) that included observed variables instead of latent variables for the Big Five. This is further evidence that the complex measurement model causes problems and that the simpler observed-variable models should be preferred. A full list of parameter estimates for the paths from personality to religion and religion to personality, along with a list of estimation and convergence problems are included in Tables 1 through 16 of the supplement. 

Because of these estimation problems, we rely only on the four observed-variable models when examining the effects of state-level religiosity on the lagged paths. Specifically, we first tested each model in each state and then used meta-analytic procedures to test whether state-level religiosity moderated these paths in each model. Parameter estimates and 95% confidence intervals are presented in Figure \@ref(fig:moderator-meta-rt) for the paths from personality to religiosity and Figure \@ref(fig:moderator-meta-tr) for the paths from religiosity to personality. 

```{r moderator-meta-rt, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Meta-analytic results for state-level religiosity as a moderator of the lagged effect of traits on religiosity"}

## Load Results
metaModResults <- read_csv("results/metaModResults.csv")

metaModResults %>%
    filter(
        effect == "rt.cl",
        type == "Observed"
    ) %>%
    ggplot(
        aes(
            x = trait,
            y = mod.est,
            ymin = mod.lb,
            ymax = mod.ub,
            color = model,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size = 2) +
    geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
    coord_flip() +
    theme_bw() +
    scale_color_grey() +
    theme(
        panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank()
        ) +
    scale_y_continuous(limits = c(-.08, .08))


```

In the original paper, @entringer_big_2023 found moderating effects of state-level religiosity for the paths from openness and conscientiousness to religiosity. Figure \@ref(fig:moderator-meta-rt) shows that both of these effects were replicated when the CLPM was used and all traits were modeled simultaneously. This was true even though we focus on the model that uses observed variables for the Big Five traits, whereas Entringer et al. modeled these as latent variables. However, as was true for the simple lagged effects, these results varied depending on precisely which model specification was used. In the case of openness, both version of the CLPM resulted in significant moderating effects, but neither version of the RI-CLPM effect did. For the conscientiousness effect, when all five traits were modeled simultaneously, both the CLPM and RI-CLPM resulted in significant moderating effects. When conscientiousness was examined on its own, however, these effects were not significant. Moderating effects for the other three traits were consistently small and nonsignificant across the four model specifications. 

@entringer_big_2023 also found moderating effects of state-level religiosity for the paths from religiosity to openness, but the paths to the other traits were not significant. Figure \@ref(fig:moderator-meta-tr) shows that this one path also emerged in our version of this model. Again, h


```{r moderator-meta-tr, echo=FALSE, message=FALSE, warning=FALSE, fig.cap = "Meta-analytic results for state-level religiosity as a moderator of the lagged effect of religiosity on traits"}

metaModResults %>%
    filter(
        effect == "tr.cl",
        type == "Observed"
    ) %>%
    ggplot(
        aes(
            x = trait,
            y = mod.est,
            ymin = mod.lb,
            ymax = mod.ub,
            color = model,
            linetype = type,
            shape = variables
        )
    ) +
    geom_point(position = position_dodge(width = 0.75), size = 2) +
    geom_errorbar(width = .05, position = position_dodge(width = 0.75)) +
    coord_flip() +
    theme_bw() +
    scale_color_grey() +
    theme(
        panel.grid.major.y = element_blank(),
        panel.grid.minor = element_blank()
    ) +
    scale_y_continuous(limits = c(-.08, .08))



```


# Disclosures

## Author Contributions

Richard E. Lucas conceptualized the study and wrote the initial analysis code and the first draft of the paper. 

Julia Rohrer wrote additional code and ran all analyses, contributed additional ideas for analyses, and contributed to writing and editing the text. 

## Conflicts of Interest

The author declares that there were no conflicts of interest with respect to the authorship or the publication of this article. 

## Prior Versions

A preprint of this paper was posted on the PsyArXiv preprint server:  .


\newpage

# References

```{r create_r-references}
papaja::r_refs(file = "r-references.bib", append = FALSE)
```

